<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Day 2 Quiz: Knowledge Check</title>

    <!-- Fonts -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;500;600;700&display=swap">

    <!-- Base Styles -->
    <link rel="stylesheet" href="../../_shared/styles.css">

    <style>
        body {
            font-family: Georgia, 'Times New Roman', serif;
            line-height: 1.7;
            background: #fafafa;
        }

        .quiz-container {
            max-width: 800px;
            margin: 0 auto;
            padding: 40px 30px;
        }

        /* Navigation with Breadcrumb */
        .global-nav {
            position: sticky;
            top: 0;
            z-index: 100;
            background: white;
            border-bottom: 1px solid #e0e0e0;
            padding: 12px 24px;
            display: flex;
            justify-content: space-between;
            align-items: center;
            font-family: 'Montserrat', sans-serif;
            font-size: 14px;
        }
        .breadcrumb {
            display: flex;
            align-items: center;
            gap: 8px;
            color: #666;
        }
        .breadcrumb a {
            color: #1D428A;
            text-decoration: none;
            font-weight: 500;
        }
        .breadcrumb a:hover { text-decoration: underline; }
        .breadcrumb .separator { color: #ccc; }
        .breadcrumb .current { color: #333; font-weight: 600; }

        .nav-pills {
            display: flex;
            gap: 8px;
        }
        .nav-pill {
            padding: 8px 16px;
            border-radius: 6px;
            text-decoration: none;
            font-weight: 600;
            font-size: 13px;
            transition: all 0.2s;
        }
        .nav-pill.active {
            background: #C8102E;
            color: white;
        }
        .nav-pill:not(.active) {
            background: #f0f0f0;
            color: #333;
        }
        .nav-pill:not(.active):hover {
            background: #e0e0e0;
        }

        /* Quiz header */
        .quiz-header {
            background: linear-gradient(135deg, #1D428A 0%, #00A9E0 100%);
            color: white;
            padding: 50px 40px;
            margin: -40px -30px 40px;
            text-align: center;
            border-radius: 0 0 20px 20px;
        }
        .quiz-header h1 {
            font-family: 'Montserrat', sans-serif;
            margin: 0 0 10px;
            font-size: 2.2rem;
        }
        .quiz-header p {
            margin: 0;
            opacity: 0.9;
        }

        /* Question card */
        .question-card {
            background: white;
            padding: 30px 35px;
            margin-bottom: 25px;
            border-radius: 12px;
            box-shadow: 0 2px 12px rgba(0,0,0,0.06);
            border-left: 5px solid #C8102E;
        }
        .question-card.answered-correct {
            border-left-color: #43B02A;
            background: #f8fff8;
        }
        .question-card.answered-wrong {
            border-left-color: #E35205;
            background: #fff8f5;
        }

        .question-number {
            font-family: 'Montserrat', sans-serif;
            font-size: 0.85rem;
            color: #888;
            text-transform: uppercase;
            letter-spacing: 1px;
            margin-bottom: 10px;
        }

        .question-text {
            font-size: 1.15rem;
            font-weight: 500;
            margin-bottom: 20px;
            color: #222;
        }

        /* Answer options */
        .options {
            display: flex;
            flex-direction: column;
            gap: 12px;
        }

        .option {
            display: flex;
            align-items: center;
            gap: 15px;
            padding: 15px 20px;
            background: #f8f9fa;
            border: 2px solid #e0e0e0;
            border-radius: 8px;
            cursor: pointer;
            transition: all 0.2s ease;
        }
        .option:hover:not(.disabled) {
            background: #f0f0f0;
            border-color: #ccc;
        }
        .option.selected {
            border-color: #C8102E;
            background: rgba(200, 16, 46, 0.05);
        }
        .option.correct {
            border-color: #43B02A;
            background: rgba(67, 176, 42, 0.1);
        }
        .option.incorrect {
            border-color: #E35205;
            background: rgba(227, 82, 5, 0.1);
        }
        .option.disabled {
            cursor: default;
        }

        .option input[type="radio"] {
            display: none;
        }

        .option-marker {
            width: 30px;
            height: 30px;
            border-radius: 50%;
            border: 2px solid #ccc;
            display: flex;
            align-items: center;
            justify-content: center;
            font-family: 'Montserrat', sans-serif;
            font-weight: 600;
            font-size: 0.9rem;
            flex-shrink: 0;
            transition: all 0.2s ease;
        }
        .option.selected .option-marker {
            border-color: #C8102E;
            background: #C8102E;
            color: white;
        }
        .option.correct .option-marker {
            border-color: #43B02A;
            background: #43B02A;
            color: white;
        }
        .option.incorrect .option-marker {
            border-color: #E35205;
            background: #E35205;
            color: white;
        }

        .option-text {
            flex: 1;
        }

        /* Feedback */
        .feedback {
            margin-top: 20px;
            padding: 20px;
            border-radius: 8px;
            display: none;
        }
        .feedback.show {
            display: block;
        }
        .feedback.correct {
            background: rgba(67, 176, 42, 0.1);
            border-left: 4px solid #43B02A;
        }
        .feedback.incorrect {
            background: rgba(227, 82, 5, 0.1);
            border-left: 4px solid #E35205;
        }
        .feedback-header {
            font-family: 'Montserrat', sans-serif;
            font-weight: 600;
            margin-bottom: 10px;
        }
        .feedback.correct .feedback-header { color: #43B02A; }
        .feedback.incorrect .feedback-header { color: #E35205; }

        /* Submit button */
        .submit-btn {
            display: block;
            width: 100%;
            padding: 15px 30px;
            background: #C8102E;
            color: white;
            border: none;
            border-radius: 8px;
            font-family: 'Montserrat', sans-serif;
            font-size: 1rem;
            font-weight: 600;
            cursor: pointer;
            transition: background 0.2s;
            margin-top: 15px;
        }
        .submit-btn:hover:not(:disabled) {
            background: #a00d24;
        }
        .submit-btn:disabled {
            background: #ccc;
            cursor: not-allowed;
        }

        /* Results */
        .results {
            background: white;
            padding: 40px;
            border-radius: 12px;
            text-align: center;
            box-shadow: 0 4px 20px rgba(0,0,0,0.1);
            display: none;
        }
        .results.show {
            display: block;
        }
        .results h2 {
            font-family: 'Montserrat', sans-serif;
            margin-top: 0;
        }
        .score {
            font-size: 4rem;
            font-family: 'Montserrat', sans-serif;
            font-weight: 700;
            color: #C8102E;
            margin: 20px 0;
        }
        .score-label {
            color: #666;
            font-size: 1.1rem;
        }

        .retry-btn {
            display: inline-block;
            padding: 12px 30px;
            background: #1D428A;
            color: white;
            border: none;
            border-radius: 6px;
            font-family: 'Montserrat', sans-serif;
            font-weight: 600;
            cursor: pointer;
            margin-top: 20px;
            text-decoration: none;
        }
        .retry-btn:hover {
            background: #152d5e;
        }

        /* Progress indicator */
        .quiz-progress {
            text-align: center;
            margin-bottom: 30px;
            font-family: 'Montserrat', sans-serif;
            color: #666;
        }
        .progress-dots {
            display: flex;
            justify-content: center;
            gap: 10px;
            margin-top: 10px;
        }
        .dot {
            width: 12px;
            height: 12px;
            border-radius: 50%;
            background: #e0e0e0;
        }
        .dot.current { background: #C8102E; }
        .dot.correct { background: #43B02A; }
        .dot.incorrect { background: #E35205; }
    </style>
</head>
<body>

    <!-- Navigation -->
    <nav class="global-nav">
        <div class="breadcrumb">
            <a href="../../../index.html">UBUS 670</a>
            <span class="separator">&rsaquo;</span>
            <a href="../../index.html">Week 1</a>
            <span class="separator">&rsaquo;</span>
            <span class="current">Day 2</span>
        </div>
        <div class="nav-pills">
            <a href="index.html" class="nav-pill">Dashboard</a>
            <a href="lecture.html" class="nav-pill">Lecture</a>
            <a href="lab.html" class="nav-pill">Lab</a>
            <a href="quiz.html" class="nav-pill active">Quiz</a>
        </div>
    </nav>

    <div class="quiz-container">

        <!-- Quiz Header -->
        <div class="quiz-header">
            <h1>Day 2: Knowledge Check</h1>
            <p>Test your understanding of prompt engineering concepts</p>
            <p style="font-size: 0.85em; opacity: 0.8; margin-top: 8px;">10 randomized questions &bull; Target: 70%+ (7/10) &bull; Retake with new questions anytime</p>
        </div>

        <!-- Progress -->
        <div class="quiz-progress">
            <span id="progress-label">Question 1 of 10</span>
            <div class="progress-dots" id="progress-dots">
                <!-- Dots generated dynamically -->
            </div>
        </div>

        <!-- Questions Container (dynamically generated) -->
        <div id="questions-container">
            <!-- Questions will be generated by JavaScript -->
        </div>

        <!-- Results -->
        <div class="results" id="results">
            <h2>Quiz Complete!</h2>
            <div class="score" id="final-score">0/10</div>
            <p class="score-label">Questions Correct</p>
            <p id="results-message"></p>
            <button class="retry-btn" onclick="location.reload()">Try Again (New Questions)</button>
            <a href="index.html" class="retry-btn" style="margin-left: 10px; background: #43B02A;">Back to Dashboard</a>
        </div>

    </div>

    <script>
        // Configuration
        const QUESTIONS_PER_QUIZ = 10;

        // Question Bank - organized by topic with variants
        const questionBank = [
            // TOPIC 1: RCTFC FRAMEWORK - WHAT IT IS (2 variants)
            {
                topic: "rctfc-framework",
                question: "What does the RCTFC framework stand for in prompt engineering?",
                options: [
                    "Role, Context, Task, Format, Constraints",
                    "Research, Compose, Test, Finalize, Check",
                    "Read, Clarify, Think, Find, Conclude",
                    "Request, Configure, Train, Filter, Complete"
                ],
                correct: 0,
                feedback: {
                    correct: "Correct! RCTFC stands for Role, Context, Task, Format, and Constraints. It's a structured framework for writing effective prompts that consistently produce high-quality AI output.",
                    incorrect: "Not quite. RCTFC stands for Role, Context, Task, Format, and Constraints. Each component helps structure your prompt for better, more consistent AI output."
                }
            },
            {
                topic: "rctfc-framework",
                question: "Which component of RCTFC defines WHO the AI should act as?",
                options: [
                    "Context",
                    "Task",
                    "Role",
                    "Constraints"
                ],
                correct: 2,
                feedback: {
                    correct: "Correct! The Role component defines WHO the AI should act as. By assigning a specific role (e.g., 'Act as a financial analyst'), you activate relevant knowledge patterns and set the expertise level.",
                    incorrect: "Not quite. The Role component defines WHO the AI should act as. It sets the persona and expertise level for the AI's response."
                }
            },

            // TOPIC 2: ROLE COMPONENT (2 variants)
            {
                topic: "role",
                question: "You want AI to review a contract. Which is the BEST role assignment?",
                options: [
                    "\"You are a helpful assistant\"",
                    "\"Act as a corporate attorney with 15 years of experience in contract law\"",
                    "\"Be smart about contracts\"",
                    "\"You are a contract robot\""
                ],
                correct: 1,
                feedback: {
                    correct: "Correct! A specific role like 'corporate attorney with 15 years of experience in contract law' activates relevant legal knowledge patterns and sets an expert-level tone. Vague roles produce vague results.",
                    incorrect: "Not quite. The best role assignment is specific: 'Act as a corporate attorney with 15 years of experience in contract law.' This activates relevant knowledge patterns and sets the appropriate expertise level."
                }
            },
            {
                topic: "role",
                question: "Why does assigning a 'Role' to an AI improve output quality?",
                options: [
                    "It makes the AI run faster by narrowing its focus",
                    "It activates relevant knowledge patterns and sets appropriate expertise level",
                    "It prevents the AI from making any errors",
                    "It changes the AI's underlying model architecture"
                ],
                correct: 1,
                feedback: {
                    correct: "Correct! Assigning a role activates relevant knowledge patterns and sets the appropriate expertise level. A 'financial analyst' role produces different vocabulary, depth, and structure than a 'marketing intern' role.",
                    incorrect: "Not quite. Assigning a role activates relevant knowledge patterns and sets the expertise level. It doesn't change speed, prevent all errors, or modify the model architecture."
                }
            },

            // TOPIC 3: CONTEXT COMPONENT (2 variants)
            {
                topic: "context",
                question: "What is the purpose of the 'Context' component in RCTFC?",
                options: [
                    "To tell the AI which programming language to use",
                    "To set the maximum length of the response",
                    "To provide background information the AI needs to generate relevant output",
                    "To specify the file format for the output"
                ],
                correct: 2,
                feedback: {
                    correct: "Correct! Context provides the background information the AI needs to generate relevant, tailored output. Without context, the AI has to guess about your situation, audience, and goals.",
                    incorrect: "Not quite. The Context component provides background information the AI needs to generate relevant output. This includes details about your situation, audience, goals, and any specific data the AI should reference."
                }
            },
            {
                topic: "context",
                question: "You're asking AI to write a marketing email for Beacon Retail. Which provides better context?",
                options: [
                    "\"Write a marketing email for our company\"",
                    "\"Beacon Retail is a mid-size retailer targeting families aged 25-45 in the Midwest. We're launching a spring clearance event with 30% off home goods. Our tone is friendly and approachable. Write a marketing email for this campaign.\"",
                    "\"Write the best email ever\"",
                    "\"Make it professional and good\""
                ],
                correct: 1,
                feedback: {
                    correct: "Correct! The detailed context (company details, target audience, campaign specifics, and tone) gives the AI everything it needs to produce a relevant, tailored email. More context leads to better output.",
                    incorrect: "Not quite. The option with company details, audience demographics, campaign specifics, and tone preferences provides the best context. Vague instructions produce generic results."
                }
            },

            // TOPIC 4: TASK AND FORMAT (2 variants)
            {
                topic: "task-format",
                question: "Which prompt uses the 'Format' component most effectively?",
                options: [
                    "\"Analyze the sales data\"",
                    "\"Give me a good analysis\"",
                    "\"Provide the analysis as a table with columns: Department, Issue, Recommended Action, Priority Level\"",
                    "\"Make it look nice\""
                ],
                correct: 2,
                feedback: {
                    correct: "Correct! Specifying a table format with exact column names tells the AI exactly HOW to structure the output. This eliminates guesswork and ensures you get results in the format you need.",
                    incorrect: "Not quite. The most effective Format specification is: 'Provide the analysis as a table with columns: Department, Issue, Recommended Action, Priority Level.' This tells the AI exactly how to structure the output."
                }
            },
            {
                topic: "task-format",
                question: "What's the difference between 'Task' and 'Format' in RCTFC?",
                options: [
                    "They are the same thing, just different words",
                    "Task is for simple prompts; Format is for complex ones",
                    "Task = WHAT to do; Format = HOW to present the output",
                    "Task is required; Format is never needed"
                ],
                correct: 2,
                feedback: {
                    correct: "Correct! Task defines WHAT you want the AI to do (e.g., 'analyze customer complaints'), while Format defines HOW to present it (e.g., 'as a numbered list with severity ratings'). Both work together for clear output.",
                    incorrect: "Not quite. Task defines WHAT you want the AI to do, while Format defines HOW to present the output. For example, the Task might be 'summarize this report' and the Format might be 'as 5 bullet points.'"
                }
            },

            // TOPIC 5: ZERO-SHOT VS FEW-SHOT (2 variants)
            {
                topic: "zero-few-shot",
                question: "What is 'zero-shot' prompting?",
                options: [
                    "When the AI fails to generate any response",
                    "Asking AI to perform a task without providing any examples",
                    "A prompting technique that requires exactly zero words",
                    "When you delete your prompt and start over"
                ],
                correct: 1,
                feedback: {
                    correct: "Correct! Zero-shot prompting means asking the AI to perform a task without providing any examples. You rely on the AI's training to understand what you want. It works well for straightforward, common tasks.",
                    incorrect: "Not quite. Zero-shot prompting means asking the AI to perform a task without providing any examples. The 'zero' refers to zero examples, not zero words or zero attempts."
                }
            },
            {
                topic: "zero-few-shot",
                question: "When should you use few-shot prompting instead of zero-shot?",
                options: [
                    "Always, because few-shot is always better",
                    "Only when the AI refuses to respond",
                    "When you need output in a specific style or format and providing examples would clarify expectations",
                    "Never, because examples confuse the AI"
                ],
                correct: 2,
                feedback: {
                    correct: "Correct! Few-shot prompting is ideal when you need output in a specific style, format, or tone. Providing 2-3 examples shows the AI exactly what you expect, reducing ambiguity and improving consistency.",
                    incorrect: "Not quite. Few-shot prompting is best when you need output in a specific style or format and providing examples would clarify your expectations. It's not always better or always worse than zero-shot."
                }
            },

            // TOPIC 6: CHAIN-OF-THOUGHT (2 variants)
            {
                topic: "chain-of-thought",
                question: "What is chain-of-thought prompting?",
                options: [
                    "Sending multiple prompts in rapid succession",
                    "Asking the AI to show its reasoning step-by-step before reaching a conclusion",
                    "Linking multiple AI models together in a chain",
                    "A technique for writing longer prompts"
                ],
                correct: 1,
                feedback: {
                    correct: "Correct! Chain-of-thought prompting asks the AI to show its reasoning step-by-step before reaching a conclusion. This improves accuracy on complex problems because the AI 'thinks through' the logic rather than jumping to an answer.",
                    incorrect: "Not quite. Chain-of-thought prompting asks the AI to reason step-by-step before concluding. It's not about sending multiple prompts, linking models, or writing longer prompts."
                }
            },
            {
                topic: "chain-of-thought",
                question: "You need AI to evaluate whether Beacon should invest in an AI chatbot. Which technique is BEST?",
                options: [
                    "Zero-shot: \"Should Beacon invest in an AI chatbot? Yes or no.\"",
                    "Few-shot: Provide examples of other companies that invested in chatbots",
                    "Chain-of-thought: \"Think through this step by step: costs, benefits, risks, then give a recommendation\"",
                    "Simple prompt: \"AI chatbot good or bad?\""
                ],
                correct: 2,
                feedback: {
                    correct: "Correct! Chain-of-thought is best for complex decisions. Asking the AI to think through costs, benefits, and risks step-by-step produces a more thorough, well-reasoned analysis than a simple yes/no request.",
                    incorrect: "Not quite. For complex business decisions, chain-of-thought prompting is best. It forces the AI to reason through each factor (costs, benefits, risks) before making a recommendation, producing deeper analysis."
                }
            },

            // TOPIC 7: PROMPT ITERATION (2 variants)
            {
                topic: "iteration",
                question: "Your AI output is too vague and generic. What's the BEST iteration strategy?",
                options: [
                    "Switch to a completely different AI tool",
                    "Add more specific constraints, context, and examples",
                    "Keep submitting the same prompt until you get better results",
                    "Make the prompt shorter and simpler"
                ],
                correct: 1,
                feedback: {
                    correct: "Correct! When output is too vague, add specificity: more detailed constraints, richer context, and concrete examples. Each iteration should make your prompt more precise about what you want.",
                    incorrect: "Not quite. When output is vague, the best strategy is to add more specific constraints, context, and examples. Repeating the same prompt or making it shorter won't help. Iteration means refining your prompt."
                }
            },
            {
                topic: "iteration",
                question: "What is the recommended approach to prompt iteration?",
                options: [
                    "Write the longest, most detailed prompt possible on the first try",
                    "Start simple, evaluate output, then add specificity incrementally",
                    "Copy prompts from the internet without modification",
                    "Ask the AI to write its own prompts"
                ],
                correct: 1,
                feedback: {
                    correct: "Correct! The best approach is iterative: start with a simple prompt, evaluate what the AI produces, then add specificity where the output falls short. This helps you identify exactly which details matter most.",
                    incorrect: "Not quite. The recommended approach is to start simple, evaluate the output, and then incrementally add specificity where needed. This iterative process is more efficient than trying to write the perfect prompt on the first attempt."
                }
            },

            // TOPIC 8: PROMPT INJECTION (2 variants)
            {
                topic: "prompt-injection",
                question: "What is 'prompt injection'?",
                options: [
                    "A technique for making prompts more effective",
                    "When a user crafts input designed to override or manipulate the AI's instructions",
                    "Adding extra context to improve AI responses",
                    "A method for injecting code into AI systems for faster processing"
                ],
                correct: 1,
                feedback: {
                    correct: "Correct! Prompt injection is a security concern where a user crafts input designed to override or manipulate the AI's original instructions. It's important to understand when building AI-powered applications.",
                    incorrect: "Not quite. Prompt injection is when a user crafts input designed to override or manipulate the AI's instructions. It's a security vulnerability, not a positive technique for improving prompts."
                }
            },
            {
                topic: "prompt-injection",
                question: "An AI customer service bot starts ignoring its guidelines after a user types 'Ignore all previous instructions.' This is an example of:",
                options: [
                    "A context window overflow",
                    "Normal AI behavior",
                    "Prompt injection attack",
                    "A hallucination"
                ],
                correct: 2,
                feedback: {
                    correct: "Correct! This is a classic prompt injection attack. The user's input ('Ignore all previous instructions') overrode the system prompt that defined the bot's behavior. This is a real security concern for AI-powered applications.",
                    incorrect: "Not quite. This is a prompt injection attack. The user deliberately crafted input to override the AI's system instructions. It's not an overflow, normal behavior, or a hallucination."
                }
            },

            // TOPIC 9: CONSTRAINTS COMPONENT (2 variants)
            {
                topic: "constraints",
                question: "Which is a well-written 'Constraints' section for a prompt?",
                options: [
                    "\"Make it good and not bad\"",
                    "\"Keep response under 200 words. Use only data from the provided report. Do not include speculative recommendations.\"",
                    "\"No constraints needed\"",
                    "\"Just do your best\""
                ],
                correct: 1,
                feedback: {
                    correct: "Correct! Good constraints are specific and actionable: word limits, data source restrictions, and content boundaries. They set clear guardrails that prevent the AI from producing unwanted content.",
                    incorrect: "Not quite. The best constraints are specific and actionable: 'Keep response under 200 words. Use only data from the provided report. Do not include speculative recommendations.' Vague statements like 'make it good' are not effective constraints."
                }
            },
            {
                topic: "constraints",
                question: "Why are constraints important in prompting?",
                options: [
                    "They make the AI respond faster",
                    "They are only needed for creative writing tasks",
                    "They prevent the AI from producing unwanted content and set boundaries on length, scope, and tone",
                    "They are optional and never improve output quality"
                ],
                correct: 2,
                feedback: {
                    correct: "Correct! Constraints set boundaries that prevent unwanted content, control response length, limit scope, and define tone. Without constraints, AI tends to produce overly long, unfocused, or off-target responses.",
                    incorrect: "Not quite. Constraints prevent the AI from producing unwanted content and set boundaries on length, scope, and tone. They're important for all types of prompts, not just creative writing."
                }
            },

            // TOPIC 10: PRACTICAL APPLICATION (2 variants)
            {
                topic: "practical",
                question: "Beacon needs to draft 50 similar job postings for seasonal workers. What prompting approach would be most efficient?",
                options: [
                    "Write each job posting prompt from scratch individually",
                    "Ask the AI to generate all 50 at once in a single prompt",
                    "Create one well-crafted RCTFC prompt as a template, then use few-shot with examples for consistency",
                    "Copy and paste a generic job posting without using AI"
                ],
                correct: 2,
                feedback: {
                    correct: "Correct! Creating a well-crafted RCTFC template with few-shot examples ensures consistency across all 50 postings while allowing for role-specific customization. This is how prompt engineering scales in practice.",
                    incorrect: "Not quite. The most efficient approach is to create one well-crafted RCTFC prompt as a template with few-shot examples for consistency. This scales your prompt engineering effort across all 50 postings."
                }
            },
            {
                topic: "practical",
                question: "You've crafted a prompt that works well for generating marketing copy. Before using it across the team, you should:",
                options: [
                    "Immediately share it with everyone without testing",
                    "Keep it to yourself since prompts are personal",
                    "Test it with multiple inputs and edge cases, then document the prompt for consistency",
                    "Delete it and let each team member write their own"
                ],
                correct: 2,
                feedback: {
                    correct: "Correct! Before scaling a prompt across a team, test it with multiple inputs and edge cases to ensure it's robust. Then document it so everyone uses it consistently. This is prompt management best practice.",
                    incorrect: "Not quite. Before sharing a prompt with the team, you should test it with multiple inputs and edge cases, then document it for consistency. Untested prompts may fail with different inputs."
                }
            }
        ];

        // Quiz state
        let selectedQuestions = [];
        let answered = 0;
        let correct = 0;

        // Initialize quiz
        function initQuiz() {
            // Select one question from each topic
            const topics = [...new Set(questionBank.map(q => q.topic))];
            selectedQuestions = [];

            topics.forEach(topic => {
                const topicQuestions = questionBank.filter(q => q.topic === topic);
                const randomQ = topicQuestions[Math.floor(Math.random() * topicQuestions.length)];
                selectedQuestions.push({...randomQ});
            });

            // Shuffle the selected questions
            selectedQuestions = shuffleArray(selectedQuestions);

            // Generate HTML for questions
            const container = document.getElementById('questions-container');
            container.innerHTML = selectedQuestions.map((q, index) => generateQuestionHTML(q, index + 1)).join('');

            // Generate progress dots
            const dotsContainer = document.getElementById('progress-dots');
            dotsContainer.innerHTML = selectedQuestions.map((_, i) =>
                `<span class="dot ${i === 0 ? 'current' : ''}"></span>`
            ).join('');

            // Add event listeners
            document.querySelectorAll('.option').forEach(option => {
                option.addEventListener('click', handleOptionClick);
            });
        }

        function shuffleArray(array) {
            const shuffled = [...array];
            for (let i = shuffled.length - 1; i > 0; i--) {
                const j = Math.floor(Math.random() * (i + 1));
                [shuffled[i], shuffled[j]] = [shuffled[j], shuffled[i]];
            }
            return shuffled;
        }

        function generateQuestionHTML(q, num) {
            // Shuffle options while tracking correct answer
            const optionsWithIndex = q.options.map((opt, i) => ({ text: opt, wasIndex: i }));
            const shuffledOptions = shuffleArray(optionsWithIndex);
            const newCorrectIndex = shuffledOptions.findIndex(o => o.wasIndex === q.correct);

            // Store the new correct index
            selectedQuestions[num - 1].correctShuffled = newCorrectIndex;

            const letters = ['A', 'B', 'C', 'D'];
            const optionsHTML = shuffledOptions.map((opt, i) => `
                <label class="option">
                    <input type="radio" name="q${num}" value="${i}">
                    <span class="option-marker">${letters[i]}</span>
                    <span class="option-text">${opt.text}</span>
                </label>
            `).join('');

            return `
                <div class="question-card" data-question="${num}" data-correct="${newCorrectIndex}">
                    <div class="question-number">Question ${num}</div>
                    <div class="question-text">${q.question}</div>
                    <div class="options">${optionsHTML}</div>
                    <div class="feedback">
                        <div class="feedback-header"></div>
                        <div class="feedback-text"></div>
                    </div>
                    <button class="submit-btn" disabled onclick="checkAnswer(${num})">Check Answer</button>
                </div>
            `;
        }

        function handleOptionClick(e) {
            const option = e.currentTarget;
            const questionCard = option.closest('.question-card');

            if (questionCard.classList.contains('answered-correct') ||
                questionCard.classList.contains('answered-wrong')) {
                return;
            }

            questionCard.querySelectorAll('.option').forEach(o => o.classList.remove('selected'));
            option.classList.add('selected');
            option.querySelector('input').checked = true;
            questionCard.querySelector('.submit-btn').disabled = false;
        }

        function checkAnswer(questionNum) {
            const card = document.querySelector(`[data-question="${questionNum}"]`);
            const correctAnswer = parseInt(card.dataset.correct);
            const selectedOption = card.querySelector('input:checked');
            const questionData = selectedQuestions[questionNum - 1];

            if (!selectedOption) return;

            const selected = parseInt(selectedOption.value);
            const isCorrect = selected === correctAnswer;

            // Disable further interaction
            card.querySelectorAll('.option').forEach((o, i) => {
                o.classList.add('disabled');
                if (parseInt(o.querySelector('input').value) === correctAnswer) {
                    o.classList.add('correct');
                }
                if (parseInt(o.querySelector('input').value) === selected && !isCorrect) {
                    o.classList.add('incorrect');
                }
            });
            card.querySelector('.submit-btn').disabled = true;
            card.querySelector('.submit-btn').textContent = isCorrect ? '\u2713 Correct' : '\u2717 Incorrect';

            // Show feedback
            const feedbackDiv = card.querySelector('.feedback');
            feedbackDiv.classList.add('show', isCorrect ? 'correct' : 'incorrect');
            feedbackDiv.querySelector('.feedback-header').textContent = isCorrect ? '\u2713 Correct!' : '\u2717 Not quite right';
            feedbackDiv.querySelector('.feedback-text').textContent = questionData.feedback[isCorrect ? 'correct' : 'incorrect'];

            // Update card styling
            card.classList.add(isCorrect ? 'answered-correct' : 'answered-wrong');

            // Update progress
            answered++;
            if (isCorrect) correct++;
            updateProgress();

            // Check if quiz complete
            if (answered === selectedQuestions.length) {
                setTimeout(showResults, 1000);
            }
        }

        function updateProgress() {
            document.getElementById('progress-label').textContent =
                `Question ${Math.min(answered + 1, selectedQuestions.length)} of ${selectedQuestions.length}`;

            const dots = document.querySelectorAll('.dot');
            dots.forEach((dot, i) => {
                dot.classList.remove('current', 'correct', 'incorrect');
                const card = document.querySelector(`[data-question="${i + 1}"]`);
                if (card.classList.contains('answered-correct')) {
                    dot.classList.add('correct');
                } else if (card.classList.contains('answered-wrong')) {
                    dot.classList.add('incorrect');
                } else if (i === answered) {
                    dot.classList.add('current');
                }
            });
        }

        function showResults() {
            document.getElementById('questions-container').style.display = 'none';
            document.querySelector('.quiz-progress').style.display = 'none';

            const results = document.getElementById('results');
            results.classList.add('show');
            document.getElementById('final-score').textContent = `${correct}/${selectedQuestions.length}`;

            let message = '';
            const percentage = (correct / selectedQuestions.length) * 100;
            if (percentage === 100) {
                message = "Perfect score! You've mastered prompt engineering concepts. You're ready to put these techniques into practice.";
            } else if (percentage >= 80) {
                message = "Great job! You passed with a strong understanding of prompt engineering. Review any questions you missed to strengthen your skills.";
            } else if (percentage >= 70) {
                message = "You passed! Solid understanding of the basics. Review the questions you missed, and remember that prompt engineering improves with practice.";
            } else {
                message = "You scored below 70%. We recommend reviewing the lecture and trying again with new randomized questions. Prompt engineering is a skill that improves with practice!";
            }
            document.getElementById('results-message').textContent = message;
        }

        // Initialize on page load
        document.addEventListener('DOMContentLoaded', initQuiz);
    </script>

</body>
</html>